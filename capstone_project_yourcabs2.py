# -*- coding: utf-8 -*-
"""Capstone Project - YOURCABS2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_XAH5VzYeIdQm6p_ev7V77MVGw41sYm7

## Importing Required Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score
from sklearn import tree
from sklearn.svm import SVC
from sklearn import metrics
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from math import pi,sin,cos,atan2,sqrt,radians

# %matplotlib inline

# supress the warnings
import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

# importing the dataset
yourcabs=pd.read_csv("/content/drive/MyDrive/Capstone Project -YOURCABS/YourCabs.csv")
yourcabs.head(5)

# peeking the data
yourcabs.shape

# Verify the data
yourcabs.info()

# Statistical Summary of your data
yourcabs.describe()

yourcabs.columns

yourcabs["travel_type_id"]=3

yourcabs["travel_type_id"]

#check the class imbalance in the data.this is a common problem in classification problem
# this tells us the our data has class imblance
yourcabs["Car_Cancellation"].value_counts(normalize=True)

"""# spliting yourcabs raw data into three models for 3 different values of travel type id for car cancellation

MODEL 1 :- travel_type_id = 3
"""

yourcabs.columns

## split the data into training and validation set
# we would use sklearn's train_test_split functionality for the same
targets=yourcabs["Car_Cancellation"]
features=yourcabs[["package_id","travel_type_id","from_area_id","from_city_id","to_city_id","from_date","online_booking","mobile_site_booking","booking_created"]]

features_train,features_val,target_train,target_val=train_test_split(features,targets,test_size=0.2,random_state=42,stratify=targets)

features_train.reset_index(drop=True,inplace=True)
features_val.reset_index(drop=True,inplace=True)
target_train.reset_index(drop=True,inplace=True)
target_val.reset_index(drop=True,inplace=True)

# only features with travel_type_id = 3
features_train["travel_type_id"] = 3
features_val["travel_type_id"] = 3

features_train.shape,features_val.shape

target_train.shape,target_val.shape

target_train.value_counts(normalize=True)

features_train.columns

"""# Data Cleaning

## Missing Value Treatment
"""

# aggregate the null count
features_train.isnull().sum()

features_train.isnull().sum()*100/ features_train.shape[0]

"""# DROPING COLUMN >= 10% MISSING VALUES / NAN VALUES"""

# droping colm
features_train.drop(["from_city_id","to_city_id","package_id"],axis=1,inplace=True)
features_val.drop(["from_city_id","to_city_id","package_id"],axis=1,inplace=True)

features_train['from_area_id'].value_counts()

features_train

# checking the null values
features_train.isnull()

# checking for unique values
features_train["travel_type_id"].nunique()

# filling nan values with mean
features_train["from_area_id"]=features_train["from_area_id"].fillna(features_train["from_area_id"].mean())
features_val["from_area_id"]=features_val["from_area_id"].fillna(features_val["from_area_id"].mean())

# checking for unique values
features_train["online_booking"].nunique()

# checking for total null values
features_train["from_area_id"].isnull().sum()

# spliting date , time col and making single colume hours
features_train["from_date"]=pd.to_datetime(features_train["from_date"])
features_train["from_date"]=features_train["from_date"].dt.hour
features_train["booking_created"]=pd.to_datetime(features_train["booking_created"])
features_train["booking_created"]=features_train["booking_created"].dt.hour
features_train["date_time"]=features_train["from_date"]-features_train["booking_created"]/24

# spliting date , time col and making single colume hours
features_val["from_date"]=pd.to_datetime(features_val["from_date"])
features_val["from_date"]=features_val["from_date"].dt.hour
features_val["booking_created"]=pd.to_datetime(features_val["booking_created"])
features_val["booking_created"]=features_val["booking_created"].dt.hour
features_val["date_time"]=features_val["from_date"]-features_val["booking_created"]/24

features_train["date_time"]

features_train

# droping col
features_train.drop(["from_date","booking_created"],axis=1,inplace=True)
features_val.drop(["from_date","booking_created"],axis=1,inplace=True)

# for describe the data statisticaly
features_train.describe()

# no of col
features_train.columns

"""# Outlier Treatment"""

# visualization of data
numeric_cols=features_train.select_dtypes(include = np.number) ### selects numeric columns
column_names=list(numeric_cols.columns)
col_index=0
plot_rows=5
plot_cols=3

fig, ax =plt.subplots(nrows=plot_rows,ncols=plot_cols,figsize=(20,20))

for row_count in range(plot_rows):
  for col_count in range(plot_cols):
    if col_index < len(column_names):
      ax[row_count][col_count].scatter(y=numeric_cols[column_names[col_index]],x=numeric_cols.index,c=target_train)
      ax[row_count][col_count].set_ylabel(column_names[col_index])
      col_index=col_index + 1
    else:

      break

plt.show()

# visualization of data using boxplot
features_train.boxplot()

"""dataset doesnt has outliers"""

features_train

features_train.isnull().sum()

"""# Statistical Summary of your data"""

# Statistical Summary of your data
features_train.describe()

"""# Correlation Analysis"""

# visualization of data using sns heatmap
sns.heatmap(features_train.corr(),annot=True,cmap="RdYlGn")

"""# Data Scaling"""

#  scaling
standard_scaler=StandardScaler()
features_train=standard_scaler.fit_transform(features_train)
features_val=standard_scaler.transform(features_val)

target_train.isnull().sum()

"""# LogisticRegression"""

# instantiating and fitting the model to training data

log_reg=LogisticRegression()
log_reg.fit(features_train,target_train)

#prediction for test data set
features_pred=log_reg.predict(features_val)
features_pred

target_val=np.array(target_val)
target_val

# accuracy
print("accuracy :",metrics.accuracy_score(target_val,features_pred))

"""# classification_report"""

# classification_report
print(classification_report(target_val,features_pred))

"""# KNeighborsClassifier"""

# knn implementation
knn=KNeighborsClassifier(n_neighbors=20,weights="distance",leaf_size=20,p=5,n_jobs=-1)
knn.fit(features_train,target_train)
#prediction for test data set
features_pred=knn.predict(features_val)
features_pred

# accuracy
print("accuracy :",metrics.accuracy_score(target_val,features_pred))

# classification_report
print(classification_report(target_val,features_pred))

"""# confusion_matrix"""

# confusion_matrix
conf_matrix=confusion_matrix(target_val,features_pred)
print(conf_matrix)

"""# DecisionTreeClassifier"""

# DecisionTreeClassifier
dt=tree.DecisionTreeClassifier(random_state=42,max_depth=26,criterion="entropy",min_samples_split=8,min_samples_leaf=3)
dt.fit(features_train,target_train)
#prediction for test data set
features_pred=dt.predict(features_val)
features_pred

# accuracy
print("accuracy :",metrics.accuracy_score(target_val,features_pred))

"""# confusion_matrix"""

## confusion_matrix
conf_matrix=confusion_matrix(target_val,features_pred)
print(conf_matrix)

# classification_report
print(classification_report(target_val,features_pred))

"""# KNeighborsClassifier"""

## knn
knn = KNeighborsClassifier(n_neighbors=3)

knn.fit(features_train,target_train)
pred = knn.predict(features_val)

print('WITH K=3')
print('\n')
print(confusion_matrix(target_val,features_pred))
print('\n')
print(classification_report(target_val,features_pred))

## knn n_n=20
knn = KNeighborsClassifier(n_neighbors=20)

knn.fit(features_train,target_train)
pred = knn.predict(features_val)

print('WITH K=20')
print('\n')
print(confusion_matrix(target_val,features_pred))
print('\n')
print(classification_report(target_val,features_pred))

"""# SVM"""

### SVC
svc=SVC(kernel="linear")
svc.fit(features_train,target_train)
features_pred=svc.predict(features_val)
print("accuracy score:")
print(metrics.accuracy_score(target_val,features_pred))

# classification_report
print(classification_report(target_val,features_pred))

## SVC
svc=SVC(kernel="sigmoid",gamma="auto",C=2.2)
svc.fit(features_train,target_train)
features_pred=svc.predict(features_val)
print("accuracy score:")
print(metrics.accuracy_score(target_val,features_pred))

# classification_report

print(classification_report(target_val,features_pred))

"""# RandomForestClassifier"""

## RandomForestClassifier
rf=RandomForestClassifier(n_estimators=200,random_state=42,max_depth=6,max_features="sqrt",criterion="entropy")
rf.fit(features_train,target_train)
features_pred=rf.predict(features_val)
print("accuracy score:")
print(metrics.accuracy_score(target_val,features_pred))

# classification_report
print(classification_report(target_val,features_pred))

## RandomForestClassifier #
rf=RandomForestClassifier(n_estimators=500,random_state=42,max_depth=8,max_features="log2",criterion="entropy")
rf.fit(features_train,target_train)
features_pred=rf.predict(features_val)
print("accuracy score:")
print(metrics.accuracy_score(target_val,features_pred))

# classification_report

print(classification_report(target_val,features_pred))

"""## HyperParameter tuning
#### GridSearchCV of sklearn helps us in performing hyperparameter tuning

# GridSearchCV FOR DecisionTreeClassifier
"""

#GridSearchCV
dt_gv=DecisionTreeClassifier()
params_grid={
    "random_state":[42],
    "max_depth":[5,6,7],
    "criterion":["gini","entropy"],
    "splitter":["best","random"],
    "min_samples_leaf":[2,4,5,6],
    "min_samples_split":[3,4,5,6,7],
    "max_features":["sqrt",0.5,0.8,0.9],
    "class_weight":[{0:1.2,1:3},{0:1.3,1:3.5},{0:1.2,1:4.5},{0:1.2,1:5},{0:0.9,1:4.5},{0:1,1:5.5}]
}

grid_search_cv=GridSearchCV(
    estimator=dt_gv,
    param_grid=params_grid,
    scoring="f1",
    cv=5,
    return_train_score=True
)
grid_search_cv.fit(features_train,target_train)

"""#GridSearchCV FOR LogisticRegression"""

#GridSearchCV

lr_gv=LogisticRegression()
params_grid={
    "max_iter":[500,1000],
    "solver":["newton-cholesky","newton-cg"],
    "C":[1.3,1.4,1.5],
    "n_jobs":[-1],
    "class_weight":[{0:1.2,1:3},{0:1.3,1:3.5},{0:1.2,1:4.5},{0:1.2,1:5}]
}

grid_search_cv=GridSearchCV(
    estimator=lr_gv,
    param_grid=params_grid,
    scoring="f1",
    cv=5,
    return_train_score=True
)
grid_search_cv.fit(features_train,target_train)

"""# GridSearchCV FOR KNeighborsClassifier"""

#GridSearchCV

knn_gv=KNeighborsClassifier()
params_grid={
    "n_neighbors":[20,21,23],
    "weights":["distance"],
    "leaf_size":[20,30,40],
    "n_jobs":[-1],
    "p":[5,6,7]
}

grid_search_cv=GridSearchCV(
    estimator=knn_gv,
    param_grid=params_grid,
    scoring="f1",
    cv=5,
    return_train_score=True
)
grid_search_cv.fit(features_train,target_train)

"""# GridSearchCV FOR RandomForestClassifier"""

# GridSearchCV

rf=RandomForestClassifier()

params_grid={
    "n_estimators":[500],
    "max_depth":[8,11,13,None],
    "min_samples_split":[50,70,100],
    "max_features":["sqrt",0.5],
    "n_jobs":[-1],
    "class_weight":[{0:1,1:3},{0:1,1:4},{0:0.9,1:3.5},{0:1.2,1:4.2}]
}

grid_search_cv = GridSearchCV(
    estimator=rf,
    param_grid=params_grid,
    scoring="f1",
    cv=5,
    return_train_score=True
)
grid_search_cv.fit(features_train,target_train)

results_df=pd.DataFrame({
    "Parameters":grid_search_cv.cv_results_["params"],
    "MeanTrainScore":grid_search_cv.cv_results_["mean_train_score"],
    "MeanTestScore":grid_search_cv.cv_results_["mean_test_score"]})

results_df.to_csv("GridSearchPerformannce_rf.csv",index=None)

# BEST KNN
#{'leaf_size': 20, 'n_jobs': -1, 'n_neighbors': 5, 'p': 7, 'weights': 'distance'}
#MeanTrainScore:-0.854157356756882
#MeanTestScore:-0.1958418194211

"""# BEST DecisionTreeClassifier"""

bestdt=DecisionTreeClassifier(class_weight={0:1.2,1:4.5},
                              max_depth=14,
                              criterion="entropy",
                              random_state=42,
                              min_samples_split=6,
                              min_samples_leaf=6,
                              max_features=0.9)
bestdt.fit(features_train,target_train)

tr_pred = bestdt.predict(features_train)
val_pred = bestdt.predict(features_val)
ConfusionMatrixDisplay.from_predictions(target_train,tr_pred)
ConfusionMatrixDisplay.from_predictions(target_val,val_pred)

train_f1 = f1_score(target_train,tr_pred)
val_f1 = f1_score(target_val,val_pred)

print("train F1 Score - {}".format(train_f1))
print("val F1 Score - {}".format(val_f1))

"""# second model :- travel_type_id = 1"""

yourcabs.columns

## split the data into training and validation set
targets1=yourcabs["Car_Cancellation"]
features1=yourcabs[["travel_type_id","from_area_id","from_city_id","to_city_id","online_booking","mobile_site_booking"]]

features_train1,features_val1,target_train1,target_val1=train_test_split(features1,targets1,test_size=0.2,random_state=42,stratify=targets1)

features_train1.reset_index(drop=True,inplace=True)
features_val1.reset_index(drop=True,inplace=True)
target_train1.reset_index(drop=True,inplace=True)
target_val1.reset_index(drop=True,inplace=True)

features_train1.shape

features_val1.shape

target_train1.value_counts()

features_train1.info()

# features with travel_type_id = 1

features_train1["travel_type_id"]=1

features_val1["travel_type_id"]=1

features_train1.isnull().sum()* 100/features_train1.shape[0]

"""# DROPING COLUMN >= 10% MISSING VALUES / NAN VALUES"""

# drop
features_train1.drop(["from_city_id","to_city_id"],axis=1,inplace=True)
features_val1.drop(["from_city_id","to_city_id"],axis=1,inplace=True)

features_train1.describe()

features_train1["travel_type_id"]=1

features_train1

features_train1["from_area_id"].fillna(features_train1["from_area_id"].mean(),inplace=True)
features_val1["from_area_id"].fillna(features_train1["from_area_id"].mean(),inplace=True)

features_train1.isnull().sum()

# visualization of data using boxplot
features_train1.boxplot()

# visualization of data using sns heatmap
sns.heatmap(features_train1[['from_area_id','online_booking','mobile_site_booking']].corr(),cmap="YlGnBu",annot=True)

"""from sklearn.preprocessing import MinMaxScaler
min_sc=MinMaxScaler()
features_train1=min_sc.fit_transform(features_train1)
features_val1=min_sc.transform(features_val1)
"""

features_train1

# feature  scaling
standard_scaler=StandardScaler()
features_train1=standard_scaler.fit_transform(features_train1)
features_val1=standard_scaler.transform(features_val1)

## LogisticRegression
lr=LogisticRegression(max_iter=500,random_state=42,class_weight={0:1,1:4.5})
lr.fit(features_train1,target_train1)
#prediction for test data set
features_pred1=lr.predict(features_val1)
features_pred1

# accuracy

print("accuracy :",metrics.accuracy_score(target_val1,features_pred1))

# classification_report
print(classification_report(target_val1,features_pred1))

#knn- KNeighborsClassifier
knn=KNeighborsClassifier(n_neighbors=5,weights="distance",leaf_size=20,p=3,n_jobs=-1)
knn.fit(features_train1,target_train1)
#prediction for test data set
features_pred1=knn.predict(features_val1)
features_pred1

# accuracy

print("accuracy :",metrics.accuracy_score(target_val1,features_pred1))

# classification_report
print(classification_report(target_val1,features_pred1))

## DecisionTreeClassifier
dt=tree.DecisionTreeClassifier(random_state=42,max_depth=5,criterion="entropy",min_samples_split=4,min_samples_leaf=2)
dt.fit(features_train1,target_train1)
#prediction for test data set
features_pred1=dt.predict(features_val1)
features_pred1

# accuracy
print("accuracy :",metrics.accuracy_score(target_val1,features_pred1))

# classification_report
print(classification_report(target_val1,features_pred1))

# Randomforestclassifier
rf=RandomForestClassifier(n_estimators=100,random_state=42,max_depth=8,max_features="sqrt")
rf.fit(features_train1,target_train1)
features_pred1=rf.predict(features_val1)
print("accuracy score:")
print(metrics.accuracy_score(target_val1,features_pred1))

# classification_report
print(classification_report(target_val1,features_pred1))

## SVC
svc=SVC()
svc.fit(features_train1,target_train1)
features_pred1=svc.predict(features_val1)
print("accuracy score:")
print(metrics.accuracy_score(target_val1,features_pred1))

# classification_report
print(classification_report(target_val1,features_pred1))

""" # GridSearchCV FOR DecisionTreeClassifier"""

#GridSearchCV

dt_gv=DecisionTreeClassifier()
params_grid={
    "random_state":[42],
    "max_depth":[5,6,7],
    "criterion":["gini","entropy"],
    "splitter":["best","random"],
    "min_samples_leaf":[2,4,5,6],
    "min_samples_split":[3,4,5,6,7],
    "max_features":["sqrt",0.5,0.8,0.9],
    "class_weight":[{0:1.2,1:3},{0:1.3,1:3.5},{0:1.2,1:4.5},{0:1.2,1:5},{0:0.9,1:4.5},{0:1,1:5.5}]
}

grid_search_cv=GridSearchCV(
    estimator=dt_gv,
    param_grid=params_grid,
    scoring="f1",
    cv=5,
    return_train_score=True
)
grid_search_cv.fit(features_train1,target_train1)

results_df=pd.DataFrame({
    "Parameters":grid_search_cv.cv_results_["params"],
    "MeanTrainScore":grid_search_cv.cv_results_["mean_train_score"],
    "MeanTestScore":grid_search_cv.cv_results_["mean_test_score"]})

results_df.to_csv("GridSearchPerformanncedtf1.csv",index=None)

"""# GridSearchCV FOR LogisticRegression"""

#GridSearchCV

lr_gv=LogisticRegression()
params_grid={
    "max_iter":[100,500],
    "solver":["liblinear","newton-cholesky","newton-cg"],
    "C":[1.3,1.4,1.5],
    "n_jobs":[-1],
    "class_weight":[None,{0:1.2,1:3},{0:1.3,1:3.5},{0:1.2,1:4.5},{0:1.2,1:5}]
}

grid_search_cv=GridSearchCV(
    estimator=lr_gv,
    param_grid=params_grid,
    scoring="f1",
    cv=5,
    return_train_score=True
)
grid_search_cv.fit(features_train1,target_train1)

results_df=pd.DataFrame({
    "Parameters":grid_search_cv.cv_results_["params"],
    "MeanTrainScore":grid_search_cv.cv_results_["mean_train_score"],
    "MeanTestScore":grid_search_cv.cv_results_["mean_test_score"]})

results_df.to_csv("GridSearchPerformannce_lrf10.csv",index=None)

"""# GridSearchCV FOR KNeighborsClassifier"""

#GridSearchCV

knn_gv=KNeighborsClassifier()
params_grid={
    "n_neighbors":[20,21,23],
    "weights":["distance"],
    "leaf_size":[20,30,40],
    "n_jobs":[-1],
    "p":[5,6,7]
}

grid_search_cv=GridSearchCV(
    estimator=knn_gv,
    param_grid=params_grid,
    scoring="f1",
    cv=5,
    return_train_score=True
)
grid_search_cv.fit(features_train1,target_train1)

results_df=pd.DataFrame({
    "Parameters":grid_search_cv.cv_results_["params"],
    "MeanTrainScore":grid_search_cv.cv_results_["mean_train_score"],
    "MeanTestScore":grid_search_cv.cv_results_["mean_test_score"]})

results_df.to_csv("GridSearchPerformannceknnf1.csv",index=None)

"""# model 3 :- travel_type_id = 2

"""

yourcabs.columns

## split the data into training and validation set
targets2=yourcabs["Car_Cancellation"]
features2=yourcabs.drop(columns=["id","user_id","vehicle_model_id","package_id","from_city_id","to_city_id","Car_Cancellation"])

features_train2,features_val2,target_train2,target_val2=train_test_split(features2,targets2,test_size=0.2,random_state=42,stratify=targets2)

features_train2.reset_index(drop=True,inplace=True)
features_val2.reset_index(drop=True,inplace=True)
target_train2.reset_index(drop=True,inplace=True)
target_val2.reset_index(drop=True,inplace=True)

# null values features with mean
features_train2["from_area_id"]=features_train2["from_area_id"].fillna(features_train2["from_area_id"].mean())
features_val2["from_area_id"]=features_val2["from_area_id"].fillna(features_val2["from_area_id"].mean())

# features with travel_type_id = 2
features_train2["travel_type_id"] = 2
features_val2["travel_type_id"] = 2

columns=["from_lat","from_long","to_lat","to_long"]
for i in columns:
  features_train2[i].fillna(value=features_train2[i].mode().iloc[0],inplace=True)

columns=["from_lat","from_long","to_lat","to_long"]
for i in columns:
  features_val2[i].fillna(value=features_val2[i].mode().iloc[0],inplace=True)

features_train2.isnull().sum()

features_train2

# long ,lat convertion
def long_lat_converter(lon1,lat1,lon2,lat2):
  R = 6378.137
  lon1,lat1,lon2,lat2= map(radians,[lon1,lat1,lon2,lat2])
  dlat = lat2-lat1
  dlon = lon2-lon1
  a = sin((dlat/2))**2 + cos(lat1*pi/180)*cos(lat2*pi/180)*(sin(dlon/2))**2
  c = 2* atan2(sqrt(a),sqrt(1-a))
  distance = R*c
  return distance
features_val2["Distace_km"]=features_val2.apply(lambda x:long_lat_converter(x["from_long"],x["from_lat"],x["to_long"],x["to_lat"]),axis=1)
features_train2["Distace_km"]=features_train2.apply(lambda x:long_lat_converter(x["from_long"],x["from_lat"],x["to_long"],x["to_lat"]),axis=1)

#  features_train2 converting date and time
features_train2["from_date"]=pd.to_datetime(features_train2["from_date"])
features_train2["from_date"]=features_train2["from_date"].dt.hour
features_train2["booking_created"]=pd.to_datetime(features_train2["booking_created"])
features_train2["booking_created"]=features_train2["booking_created"].dt.hour
features_train2["date_time"]=features_train2["from_date"]-features_train2["booking_created"]/24

#  features_val2 converting date and time
features_val2["from_date"]=pd.to_datetime(features_val2["from_date"])
features_val2["from_date"]=features_val2["from_date"].dt.hour
features_val2["booking_created"]=pd.to_datetime(features_val2["booking_created"])
features_val2["booking_created"]=features_val2["booking_created"].dt.hour
features_val2["date_time"]=features_val2["from_date"]-features_val2["booking_created"]/24

# droping unrequerd columns
features_train2.drop(["from_date","to_area_id","booking_created","from_lat","from_long","to_lat","to_long"],axis=1,inplace=True)
features_val2.drop(["from_date","to_area_id","booking_created","from_lat","from_long","to_lat","to_long"],axis=1,inplace=True)

# taking null values into % percentage
features_train2.isnull().sum()* 100/features_train2.shape[0]

# shape of training / val
features_train2.shape,features_val2.shape

# # visualization of data using boxplot
features_train2.boxplot()

# visualization of data using subplot
numeric_cols=features_train2.select_dtypes(include = np.number) ### selects numeric columns
column_names=list(numeric_cols.columns)
col_index=0
plot_rows=5
plot_cols=3

fig, ax =plt.subplots(nrows=plot_rows,ncols=plot_cols,figsize=(20,20))

for row_count in range(plot_rows):
  for col_count in range(plot_cols):
    if col_index < len(column_names):
      ax[row_count][col_count].scatter(y=numeric_cols[column_names[col_index]],x=numeric_cols.index,c=target_train2)
      ax[row_count][col_count].set_ylabel(column_names[col_index])
      col_index=col_index + 1
    else:

      break

plt.show()

features_train2.isnull().sum()

# % of null values
features_val2.isnull().sum()*100/features_val2.shape[0]

"""# feature  scaling"""

# feature  scaling
standard_scaler=StandardScaler()
features_train2=standard_scaler.fit_transform(features_train2)
features_val2=standard_scaler.transform(features_val2)

"""# LogisticRegression"""

# LogisticRegression
lr=LogisticRegression(max_iter=500,random_state=42,class_weight={0:1,1:4.5})
lr.fit(features_train2,target_train2)
#prediction for test data set
features_pred2=lr.predict(features_val2)
features_pred2

"""# accuracy"""

# accuracy
print("accuracy :",metrics.accuracy_score(target_val2,features_pred2))

"""# classification_report"""

# classification_report
print(classification_report(target_val2,features_pred2))

"""#knn- KNeighborsClassifier"""

#knn- KNeighborsClassifier
knn=KNeighborsClassifier(n_neighbors=25,weights="uniform",leaf_size=25,p=6,n_jobs=-1)
knn.fit(features_train2,target_train2)
#prediction for test data set
features_pred2=knn.predict(features_val2)
features_pred2

# accuracy metrics
print("accuracy :",metrics.accuracy_score(target_val2,features_pred2))

## classification_report
print(classification_report(target_val2,features_pred2))

"""# DecisionTreeClassifier"""

# DecisionTreeClassifier
dt=tree.DecisionTreeClassifier(random_state=42,max_depth=6,criterion="entropy",min_samples_split=8,min_samples_leaf=3)
dt.fit(features_train2,target_train2)
#prediction for test data set
features_pred2=dt.predict(features_val2)
features_pred2

# accuracy
from sklearn import metrics
print("accuracy :",metrics.accuracy_score(target_val2,features_pred2))

# classification_report
print(classification_report(target_val2,features_pred2))

"""# RandomForestClassifier"""

# RandomForestClassifier
rf=RandomForestClassifier(n_estimators=400,random_state=42,max_depth=9,max_features="sqrt",criterion="entropy")
rf.fit(features_train2,target_train2)
features_pred2=rf.predict(features_val2)
print("accuracy score:")
print(metrics.accuracy_score(target_val2,features_pred2))

# classification_report
print(classification_report(target_val2,features_pred2))

"""# SVM"""

## SVC
svc=SVC()
svc.fit(features_train2,target_train2)
features_pred2=svc.predict(features_val2)
print("accuracy score:")
print(metrics.accuracy_score(target_val2,features_pred2))

## classification_report
print(classification_report(target_val2,features_pred2))

"""### The Goal Of the competition is to create a predictive model for classifying new bookings as to whether they will eventually gets cancelled due to car unavailability company does not have to pay a penality to the customer."""